INFO:/home/dimweb/Desktop/deeppavlov/FoCus/train_focus.py:Arguments: Namespace(model_name='GPT2', gpt2_model_path='gpt2', bart_model_path='facebook/bart-base', train_dataset_path='data/train_focus.json', train_dataset_cache='data/focus_cache.tar.gz', dev_dataset_path='data/valid_focus.json', dev_dataset_cache='data/focus_cache.tar.gz', ps_coef=1.0, kn_coef=1.0, lm_coef=10.0, max_history=1, train_batch_size=4, valid_batch_size=1, gradient_accumulation_steps=16, lr=6.25e-05, max_norm=1.0, n_epochs=2, eval_before_start=False, inference=False, test_infer=False, device='cuda', fp16='', local_rank=-1, gpu_start_num=1, flag='E2_L10', debug=True, seed=19950604, random_knowledge=False, incontext=True)
INFO:/home/dimweb/Desktop/deeppavlov/FoCus/train_focus.py:Prepare tokenizer, pretrained model and optimizer.
Some weights of GPT2PK_ctxt were not initialized from the model checkpoint at gpt2 and are newly initialized: ['attn1.weight', 'attn2.bias', 'concat_summary.summary.weight', 'summary.summary.bias', 'summary.summary.weight', 'attn2.weight', 'concat_summary.summary.bias', 'attn1.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/dimweb/Desktop/deeppavlov/d_env/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
INFO:/home/dimweb/Desktop/deeppavlov/FoCus/train_focus.py:Prepare datasets
INFO:/home/dimweb/Desktop/deeppavlov/FoCus/utils_focus.py:Process dataset from data/train_focus.json
